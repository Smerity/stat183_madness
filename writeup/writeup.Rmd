  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>March Madness: Andrew, Kevin, Smerity</title>
  <link href="//netdna.bootstrapcdn.com/bootswatch/3.0.2/yeti/bootstrap.min.css" rel="stylesheet">
  </head>

  <body>

  <div class="container">

## March Madness
<h4 class="text-muted">Andrew, Kevin, Smerity</h4>
## Week 3

<hr />

This week of the competition involved preparing for the 2014 data, rewriting the feature extraction framework and realising (in hindsight) obvious mistakes that severely limited our accuracy involving "Regular vs Tourney" and "Leave One (Season) Out".

## Reproducing the results

If you have the `rPython` library and the relevant data files (all available from the forums), reproducing our work involves `Rscript featureExtraction.R`, then `python extract_feats.py`. Unfortunately the files produced in `temp` need to have a header name changed from `id` to `team`.
To prevent you doing this, pregenerated files have been provided.

Finally, run `Rscript predict.R`.

For the data files, all required files have been supplied.
They are either in an R readable format or CSV.

The total running time for the steps above is minimal on a modern computer and requires little memory.

## "Regular vs Tourney" and "Leave One (Season) Out"

Using regular season results, although tempting, severely impacts the accuracy of the predictions.
This was a surprise as generally more data is better data.
Using regular season results, it is very difficult to get below around 0.50.

There are far fewer of them but they result in a far better prediction.
Regular season results could likely be integrated in some way (such as using them for an initial classifier and then using that output as a feature to the tourney model) but for now we haven't pursued this.

To "make" as many examples as possible, each `match(A, B)` was also entered as `match(B, A)`.

Finally, the accuracy via the "leave one season out" is substantially better than the train / test split, likely now due to the extremely small data size.
This is the difference between 0.40 and 0.25.
Unfortunately, we didn't have time to implement "leave one season out", though luckily this won't impact use for the 2014 results as we will use all previous seasons as training.

## Rewriting the feature extraction framework

The older feature extraction framework was not simple.
To fix this, we've created a "framework" that allows for the easy integration of any (season, team) data via an existing Python script.

Given a CSV file "data/pointspread.csv" with rows "team, season, x, y, z, ..." a user can select the row `y` and `z` to be used from the file by adding a single line:

`("data/pointspread.csv", ["y" "z"])`

This makes it easier to add or subtract features from a given model.
This feature extraction script also takes next to no memory compared to the previous scripts that were more memory intensive.

  </div>
  </body>
</html>
