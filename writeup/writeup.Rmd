  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>March Madness: Andrew, Kevin, Smerity</title>
  <link href="//netdna.bootstrapcdn.com/bootswatch/3.0.2/yeti/bootstrap.min.css" rel="stylesheet">
  </head>

  <body>

  <div class="container">

## March Madness
<h4 class="text-muted">Andrew, Kevin, Smerity</h4>
## Week 2

<hr />

For this week of the competition, our primary focus was on retrieving new data and experimenting with different machine learning models in an aim to avoid overfitting.

## Reproducing our work

If you have the `rPython` library and the relevant data files (all available from the forums), reproducing our work is as simple as running `Rscript featureExtraction.R` followed by `Rscript predict.R`.

For the data files, download these and place them in the `data` folder.
Two pregenerated files in R format have been supplied.

+ [ordinal_ranks_core_33.csv](https://www.kaggle.com/c/march-machine-learning-mania/forums/t/6770/extra-data-ordinal-ranks-from-kenneth-massey)
+ [rpi.csv](https://www.kaggle.com/c/march-machine-learning-mania/forums/t/6769/calculating-rpi-a-detailed-example)
+ [chessmetrics.csv](https://www.kaggle.com/c/march-machine-learning-mania/forums/t/6777/about-the-chessmetrics-benchmark)

The total running time for the steps above is minimal on a modern computer. Do note, however, this code is not memory efficient and may get a little angry if you've too many browser windows.

The only slight addendum is that the final file, `temp/final.csv`, needs to have the heading fixed to `id,prev`.

## Details of our work

### GLMNet for avoiding overfitting

Our primary focus this week has been using and analyzing the results of a model which avoids overfitting on the dataset.
This overfitting can be insidious as the user may be tempted to check their results on the final submission predictions, subtly tricking themselves into tweaking the model to work better over that set than it should, poisoning their features and model for the upcoming 2014 prediction.

To solve this, we turned to L2 regularization on the GLM model.
L2 regularization encourages the feature weights to be small, removing overreliance on features that fit too well.
We also experimented with L1 regularization and lasso regularization (L1 and L2).
There are no incredibly positive results from these experiments as L1 regularization encourages features to go to zero, even if they may contribute a little.
As processing features is not a bottleneck for computation time and we already have the data readily available, minimizing the total number of features used is not important.

For the selection of the lambda values for L2 regularization, we use the built-in cross validation offered by the GLMNet library.
The cross validation adds little computational overhead and only brings down the final prediction accuracy by a small amount.
Whilst that might seem a negative (reducing the final prediction accuracy), it gives us an assurance that if the primary feature of the model has less predictive power in our 2014 data that we aren't caught out entirely.

The old adage of "don't put all your eggs in one basket" holds just as well in machine learning.

  </div>
  </body>
</html>